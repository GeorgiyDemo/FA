{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5669,
     "status": "ok",
     "timestamp": 1619632510103,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "zKMq7dp2W15Y",
    "outputId": "ce2273c5-6a96-4216-9d88-fbee51bf5ff0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as t\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24499,
     "status": "ok",
     "timestamp": 1619632529945,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "hBkqaK5bawXN",
    "outputId": "9a2f2795-0b30-4d49-cf0c-286c5a6da777"
   },
   "outputs": [],
   "source": [
    "def common_train(\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        train_dataloader: DataLoader,\n",
    "        epochs: int,\n",
    "        test_dataloader: DataLoader = None,\n",
    "        lr_scheduler=None,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"mps\",\n",
    ") -> t.List[float]:\n",
    "    \"\"\"\n",
    "    Функция common_train() обучает модель nn.Module с использованием функции потерь nn.Module,\n",
    "    оптимизатора optim.Optimizer и даталоадера для обучающего набора данных train_dataloader на\n",
    "    заданное число эпох epochs.\n",
    "    \n",
    "    Опционально, может быть передан также даталоадер для тестового набора данных test_dataloader и шедулер\n",
    "    обучения с убыванием скорости обучения lr_scheduler.\n",
    "    Функция возвращает список значений функции потерь на каждой эпохе.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 32)\n",
    "        train_loss = train_loop(\n",
    "            train_dataloader,\n",
    "            model,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "        )\n",
    "        train_losses.append(train_loss.item())\n",
    "        if test_dataloader:\n",
    "            loss, acc = test_loop(test_dataloader, model, loss_fn, device=device)\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step(loss)\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        verbose: int = 100,\n",
    "        device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Функция train_loop() выполняет обучение модели на одной эпохе с использованием\n",
    "    даталоадера обучающего набора данных dataloader, модели nn.Module,\n",
    "    функции потерь nn.Module и оптимизатора optim.Optimizer.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)  # noqa\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{batch * len(x):>5d}/{size:>5d}]\")\n",
    "\n",
    "        del x, y, pred, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return avg_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_loop(\n",
    "        dataloader: DataLoader,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        device: str = \"cpu\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Функция test_loop() выполняет тестирование модели с использованием\n",
    "    даталоадера тестового набора данных dataloader,\n",
    "    модели nn.Module и функции потерь nn.Module. Функция \n",
    "    возвращает среднее значение функции потерь и точность на тестовом наборе данных.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss, correct = 0, 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        avg_loss += loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # noqa\n",
    "\n",
    "        del x, y, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:>4f}, Avg loss: {avg_loss:>8f} \\n\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_test_split(dataset: t.Union[Dataset, t.Sized], train_part: float) -> t.Tuple[Subset, Subset]:\n",
    "    \"\"\"\n",
    "    Функция train_test_split() разбивает набор данных dataset на обучающий и\n",
    "    тестовый наборы с использованием заданной доли обучающего набора train_part.\n",
    "    Функция возвращает кортеж, содержащий обучающий и тестовый наборы данных.\n",
    "    \"\"\"\n",
    "    train_size = round(train_part * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, lengths=(train_size, test_size))\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_y_test_y_pred(\n",
    "        model: nn.Module,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str = \"mps\",\n",
    ") -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(1)\n",
    "        y_test.append(y)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.hstack(y_test).detach().cpu(), torch.hstack(y_pred).detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm-QilGISxkt"
   },
   "source": [
    "## 1. Классификация фамилий (RNN)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/frNchuaBQVLxyA?w=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdPr92i6k-If"
   },
   "source": [
    "1.1 Используя класс `nn.RNNCell` (абстракцию для отдельного временного шага RNN), реализуйте простейшую рекуррентную сеть Элмана в виде класса `RNN`. Используя созданный класс `RNN`, решите задачу классификации фамилий. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ir6UUkl6l4tp"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Этот класс реализует обычную рекуррентную сеть (RNN) с использованием стандартного модуля nn.RNNCell из PyTorch.\n",
    "    В конструкторе класса задается размер входных данных (input_size) и размер скрытого состояния (hidden_size),\n",
    "    а также создается экземпляр nn.RNNCell с указанными размерами входных данных и скрытого состояния.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, hx: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        В методе forward реализуется сама RNN.\n",
    "        Входные данные (inputs) имеют размерность batch_size x sequence_size x input_size,\n",
    "        что означает, что в одном батче представлено sequence_size штук входных данных размера input_size.\n",
    "        Переменная hx содержит начальное состояние скрытого слоя (если не задано, то создается тензор нулей размера batch_size x hidden_size).\n",
    "        Также данные inputs транспонируются, чтобы в одной итерации цикла обрабатывался один такт времени.\n",
    "        \"\"\"\n",
    "        batch_size, sequence_size, _ = inputs.size()\n",
    "        inputs = inputs.permute(1, 0, 2)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(batch_size, self.hidden_size, dtype=inputs.dtype, device=inputs.device)\n",
    "        else:\n",
    "            hx = hx.squeeze(0)\n",
    "\n",
    "        hidden = []\n",
    "        for i in range(sequence_size):\n",
    "            hx = self.rnn_cell(inputs[i], hx)\n",
    "            hidden.append(hx)\n",
    "\n",
    "        hidden = torch.stack(hidden)\n",
    "        hx = hidden[-1].unsqueeze(0)\n",
    "        return hidden.permute(1, 0, 2), hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6515,  0.5430,  0.4023,  0.6325, -0.6068],\n",
       "          [ 0.9149, -0.1088,  0.6385, -0.7387,  0.7532],\n",
       "          [-0.6936,  0.5123, -0.2784, -0.5693, -0.0055]],\n",
       " \n",
       "         [[ 0.1954,  0.6152,  0.2958, -0.8005,  0.8074],\n",
       "          [-0.4577,  0.7566,  0.2972, -0.8834,  0.1265],\n",
       "          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n",
       "        grad_fn=<PermuteBackward0>),\n",
       " tensor([[[-0.6936,  0.5123, -0.2784, -0.5693, -0.0055],\n",
       "          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n",
       "        grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cоздаются входные данные (inputs) и начальное скрытое состояние (hx) случайным образом с помощью функции torch.randn.\n",
    "Далее создается экземпляр класса RNN с заданными размерами входных данных (input_size) и скрытого состояния (hidden_size).\n",
    "\n",
    "Также создается экземпляр стандартного модуля nn.RNN с такими же размерами, но с параметром batch_first=True, что означает,\n",
    "что размерность батча находится на первом месте (а не на втором, как у входных данных inputs).\n",
    "\"\"\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "input_size, hidden_size = 4, 5\n",
    "inputs = torch.randn(2, 3, input_size)\n",
    "hx = torch.randn(1, 2, hidden_size)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "my_rnn = RNN(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "true_rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "# Функция рассчитывает скрытые состояния и выходные данные RNN для указанных входных данных и начального скрытого состояния.\n",
    "my_rnn(inputs, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6515,  0.5430,  0.4023,  0.6325, -0.6068],\n",
       "          [ 0.9149, -0.1088,  0.6385, -0.7387,  0.7532],\n",
       "          [-0.6936,  0.5123, -0.2784, -0.5693, -0.0055]],\n",
       " \n",
       "         [[ 0.1954,  0.6152,  0.2958, -0.8005,  0.8074],\n",
       "          [-0.4577,  0.7566,  0.2972, -0.8834,  0.1265],\n",
       "          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n",
       "        grad_fn=<TransposeBackward1>),\n",
       " tensor([[[-0.6936,  0.5123, -0.2784, -0.5693, -0.0055],\n",
       "          [ 0.7166,  0.1516,  0.8047, -0.2007,  0.8192]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_rnn(inputs, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesRNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Этот класс реализует классификатор фамилий с использованием рекуррентной сети (RNN).\n",
    "    В конструкторе класса задается количество эмбеддингов (num_embeddings), размерность эмбеддинга (embedding_dim),\n",
    "    размер скрытого состояния RNN (rnn_hidden_size), размерность вектора (vector_size) и количество классов (num_classes).\n",
    "\n",
    "    В классе создается экземпляр слоя эмбеддинга (nn.Embedding) с указанными размерами и индексом для нулевых значений (padding_idx=0). \n",
    "    Также создается экземпляр класса RNN с указанными размерами эмбеддинга и скрытого состояния.\n",
    "    \n",
    "    Далее создается сеть классификации с последовательностью слоев: первый слой линейной трансформации (nn.Linear),\n",
    "    следующий слой ReLU-нелинейности (nn.ReLU), слой Dropout (nn.Dropout) и последний слой линейной трансформации для\n",
    "    получения выходных данных.\n",
    "    \n",
    "    В функции forward входные данные x преобразуются с помощью слоя эмбеддинга, затем передаются в рекуррентную сеть\n",
    "    (self.rnn) с текущим скрытым состоянием (self.hx). Полученные скрытые состояния и выходные данные сворачиваются в один вектор\n",
    "    с помощью функции torch.flatten, а затем передаются в сеть классификации.\n",
    "    \n",
    "    Функция forward возвращает выходные данные сети классификации.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            vector_size: int,\n",
    "            num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.hx = None\n",
    "        self.rnn = RNN(input_size=embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size * vector_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x, hx = self.rnn(x, self.hx)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesVocab:\n",
    "    \"\"\"\n",
    "    Класс SurnamesVocab представляет собой словарь для кодирования/декодирования фамилий.\n",
    "    \n",
    "    В конструкторе класса (__init__) передается список surnames, содержащий фамилии. Они приводятся к нижнему регистру,\n",
    "    а затем уникальные символы в них добавляются в атрибут alphabet класса, а максимальная длина фамилий сохраняется в max_len.\n",
    "    Также создается словарь ch2i, в котором каждому символу из alphabet соответствует его индекс.    \n",
    "    \"\"\"\n",
    "    pad = \"\"\n",
    "\n",
    "    def __init__(self, surnames: t.List[str]):\n",
    "        uniques = set()\n",
    "        max_len = 0\n",
    "        for w in map(str.lower, surnames):\n",
    "            uniques.update(w)\n",
    "            max_len = max(len(w), max_len)\n",
    "\n",
    "        self.alphabet = [self.pad, *uniques]\n",
    "        self.max_len = max_len\n",
    "        self.ch2i = {ch: i for i, ch in enumerate(self.alphabet)}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Метод __len__ возвращает длину alphabet.\n",
    "        \"\"\"\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Метод encode принимает фамилию word и возвращает ее кодированное представление в виде тензора типа long.\n",
    "        Для этого сначала создается список indices, содержащий индексы символов фамилии в alphabet.\n",
    "        Затем список indices дополняется символами-заполнителями (pad), чтобы добиться длины max_len.\n",
    "        \"\"\"\n",
    "        indices = [self.ch2i[ch] for ch in word]\n",
    "        indices += [self.ch2i[self.pad]] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Метод decode принимает тензор indices и возвращает фамилию, которую он кодирует.\n",
    "        Сначала находится индексы символов-заполнителей в тензоре indices с помощью функции torch.nonzero.\n",
    "        Если такие индексы есть, то indices усекается до первого такого индекса.\n",
    "        Затем фамилия создается из символов, соответствующих индексам в indices, используя alphabet.\n",
    "        \"\"\"\n",
    "        pad_indices = torch.nonzero(indices == self.ch2i[self.pad], as_tuple=True)[0]\n",
    "        if len(pad_indices):\n",
    "            indices = indices[:pad_indices[0]]\n",
    "        return \"\".join(self.alphabet[i] for i in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Этот код реализует класс SurnamesDataset, который наследует от Dataset из библиотеки PyTorch.\n",
    "    Он предназначен для хранения данных из CSV-файла, который содержит фамилии и национальности соответствующих людей.\n",
    "    \"\"\"\n",
    "    df: pd.DataFrame\n",
    "    surnames: t.List[str]\n",
    "    vocab: SurnamesVocab\n",
    "    labeler: LabelEncoder\n",
    "    data: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "\n",
    "    def __init__(self, path: Path):\n",
    "        \"\"\"\n",
    "        В методе __init__ класса считывается CSV-файл, сохраняются фамилии в список self.surnames,\n",
    "        создается экземпляр класса SurnamesVocab для кодирования/декодирования фамилий, размер тензора,\n",
    "        который будет возвращать SurnamesVocab, сохраняется в size, и создается тензор data с кодированными фамилиями.\n",
    "        Затем создается экземпляр LabelEncoder из библиотеки scikit-learn для кодирования национальностей в целочисленные значения,\n",
    "        и сохраняется результат в тензор self.targets.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "        self.surnames = self.df[\"surname\"].tolist()\n",
    "        self.vocab = SurnamesVocab(self.surnames)\n",
    "        size = self.vocab.encode(self.surnames[0].lower()).size()\n",
    "        data = torch.vstack([self.vocab.encode(w.lower()) for w in self.surnames])\n",
    "        self.data = data.view(len(self.surnames), *size)\n",
    "\n",
    "        self.labeler = LabelEncoder()\n",
    "        targets = self.labeler.fit_transform(self.df[\"nationality\"])\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Метод __len__ возвращает количество фамилий в датасете\"\"\"\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Метод __getitem__ возвращает i-ую запись (кодированную фамилию и национальность) в виде кортежа.\n",
    "        \"\"\"\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    def encode(self, word: str) -> torch.Tensor:\n",
    "        \"\"\"Метод encode кодирует фамилию в тензор, а метод decode декодирует тензор с индексами символов в строку.\"\"\"\n",
    "        return self.vocab.encode(word)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        \"\"\"Декодирует тензор с индексами символов в строку.\"\"\"\n",
    "        return self.vocab.decode(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10980"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames_dataset = SurnamesDataset(\"./data/surnames.csv\")\n",
    "len(surnames_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8784 2196\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_surnames_dataset, test_surnames_dataset = train_test_split(surnames_dataset, train_part=0.8)\n",
    "print(len(train_surnames_dataset), len(test_surnames_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "handmade_rnn_net = SurnamesRNNClassifier(\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(\"mps\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(handmade_rnn_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.897021  [    0/ 8784]\n",
      "loss: 1.476651  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.624317, Avg loss: 1.305790 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.264221  [    0/ 8784]\n",
      "loss: 1.249909  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.695811, Avg loss: 1.065277 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 0.938034  [    0/ 8784]\n",
      "loss: 1.174979  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.717668, Avg loss: 0.971192 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.887203  [    0/ 8784]\n",
      "loss: 0.913490  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.730874, Avg loss: 0.904017 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.603007  [    0/ 8784]\n",
      "loss: 0.776868  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.745902, Avg loss: 0.875295 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.732690  [    0/ 8784]\n",
      "loss: 0.600548  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.754098, Avg loss: 0.859181 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.696304  [    0/ 8784]\n",
      "loss: 0.477242  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.755009, Avg loss: 0.855929 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.429577  [    0/ 8784]\n",
      "loss: 0.613839  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.758652, Avg loss: 0.844705 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.484240  [    0/ 8784]\n",
      "loss: 0.657229  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.756831, Avg loss: 0.848067 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.350660  [    0/ 8784]\n",
      "loss: 0.502709  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.758197, Avg loss: 0.841914 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.389545  [    0/ 8784]\n",
      "loss: 0.437798  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.761384, Avg loss: 0.852533 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.391603  [    0/ 8784]\n",
      "loss: 0.433041  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.760474, Avg loss: 0.881822 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.465491  [    0/ 8784]\n",
      "loss: 0.293232  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.762750, Avg loss: 0.888703 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.415114  [    0/ 8784]\n",
      "loss: 0.348062  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.765938, Avg loss: 0.918217 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.317375  [    0/ 8784]\n",
      "loss: 0.444607  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.771858, Avg loss: 0.880585 \n",
      "\n",
      "CPU times: user 20.7 s, sys: 4.74 s, total: 25.4 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=handmade_rnn_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.97      1.00      0.98       340\n",
      "     Chinese       0.70      0.74      0.72        38\n",
      "       Czech       0.55      0.34      0.42        96\n",
      "       Dutch       0.52      0.43      0.47        51\n",
      "     English       0.74      0.86      0.80       573\n",
      "      French       0.21      0.08      0.11        39\n",
      "      German       0.51      0.43      0.47       121\n",
      "       Greek       0.62      0.74      0.68        34\n",
      "       Irish       0.79      0.30      0.43        37\n",
      "     Italian       0.76      0.75      0.75       128\n",
      "    Japanese       0.83      0.92      0.87       156\n",
      "      Korean       0.33      0.30      0.32        10\n",
      "      Polish       0.56      0.58      0.57        26\n",
      "  Portuguese       0.25      0.11      0.15         9\n",
      "     Russian       0.86      0.88      0.87       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.44      0.48      0.46        50\n",
      "  Vietnamese       1.00      0.08      0.14        13\n",
      "\n",
      "    accuracy                           0.77      2196\n",
      "   macro avg       0.59      0.50      0.51      2196\n",
      "weighted avg       0.75      0.77      0.76      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(handmade_rnn_net, test_dataloader, \"mps\")\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Замените модуль `RNN` из 1.1 на модули `nn.RNN`, `nn.LSTM` и `nn.GRU` (не забудьте указать аргумент `batch_first=True`). Сравните результаты работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesAutobotRNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            rnn_cls: t.Union[t.Type[nn.RNN], t.Type[nn.LSTM], t.Type[nn.GRU]],\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            rnn_hidden_size: int,\n",
    "            vector_size: int,\n",
    "            num_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.hx = None\n",
    "        self.rnn = rnn_cls(input_size=embedding_dim, hidden_size=rnn_hidden_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size * vector_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x, hx = self.rnn(x, self.hx)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "rnn_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.RNN,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(\"mps\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.917136  [    0/ 8784]\n",
      "loss: 1.642190  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.574681, Avg loss: 1.475679 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.348413  [    0/ 8784]\n",
      "loss: 1.366360  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.652095, Avg loss: 1.204014 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.208325  [    0/ 8784]\n",
      "loss: 1.341371  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.677596, Avg loss: 1.105765 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 0.950162  [    0/ 8784]\n",
      "loss: 1.117557  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.699909, Avg loss: 1.030863 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 0.723195  [    0/ 8784]\n",
      "loss: 1.033264  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.712204, Avg loss: 0.989619 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.875449  [    0/ 8784]\n",
      "loss: 0.763817  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.713115, Avg loss: 0.965103 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.692767  [    0/ 8784]\n",
      "loss: 0.666473  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.720856, Avg loss: 0.953870 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.477967  [    0/ 8784]\n",
      "loss: 0.829171  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.724954, Avg loss: 0.949620 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.684002  [    0/ 8784]\n",
      "loss: 0.722643  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.732240, Avg loss: 0.933952 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.490409  [    0/ 8784]\n",
      "loss: 0.654429  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.741803, Avg loss: 0.936677 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.624907  [    0/ 8784]\n",
      "loss: 0.510782  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.736794, Avg loss: 0.927090 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.544438  [    0/ 8784]\n",
      "loss: 0.540841  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.721311, Avg loss: 0.942984 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.617193  [    0/ 8784]\n",
      "loss: 0.399607  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.736794, Avg loss: 0.933420 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.531657  [    0/ 8784]\n",
      "loss: 0.373636  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.744991, Avg loss: 0.942533 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.436124  [    0/ 8784]\n",
      "loss: 0.532085  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.736339, Avg loss: 0.948486 \n",
      "\n",
      "CPU times: user 1min 57s, sys: 26.2 s, total: 2min 23s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=rnn_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.95      1.00      0.97       340\n",
      "     Chinese       0.72      0.74      0.73        38\n",
      "       Czech       0.46      0.27      0.34        96\n",
      "       Dutch       0.67      0.31      0.43        51\n",
      "     English       0.67      0.85      0.75       573\n",
      "      French       0.06      0.03      0.04        39\n",
      "      German       0.53      0.36      0.43       121\n",
      "       Greek       0.58      0.53      0.55        34\n",
      "       Irish       0.69      0.24      0.36        37\n",
      "     Italian       0.60      0.69      0.64       128\n",
      "    Japanese       0.82      0.90      0.86       156\n",
      "      Korean       0.25      0.30      0.27        10\n",
      "      Polish       0.64      0.27      0.38        26\n",
      "  Portuguese       0.00      0.00      0.00         9\n",
      "     Russian       0.84      0.85      0.85       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.52      0.34      0.41        50\n",
      "  Vietnamese       0.50      0.08      0.13        13\n",
      "\n",
      "    accuracy                           0.74      2196\n",
      "   macro avg       0.53      0.43      0.45      2196\n",
      "weighted avg       0.71      0.74      0.71      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(rnn_net, test_dataloader, \"mps\")\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "lstm_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.LSTM,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(\"mps\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 2.903144  [    0/ 8784]\n",
      "loss: 1.855295  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.530510, Avg loss: 1.642389 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 1.842572  [    0/ 8784]\n",
      "loss: 1.509554  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.611566, Avg loss: 1.375607 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 1.320147  [    0/ 8784]\n",
      "loss: 1.186791  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.650729, Avg loss: 1.203663 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 1.066090  [    0/ 8784]\n",
      "loss: 1.112567  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.677140, Avg loss: 1.111108 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 1.030352  [    0/ 8784]\n",
      "loss: 1.086272  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.695811, Avg loss: 1.043684 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 0.940045  [    0/ 8784]\n",
      "loss: 0.881155  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.713570, Avg loss: 0.987410 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 0.813504  [    0/ 8784]\n",
      "loss: 0.778378  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.717213, Avg loss: 0.961979 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 0.858059  [    0/ 8784]\n",
      "loss: 0.809305  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.726776, Avg loss: 0.942198 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 0.578364  [    0/ 8784]\n",
      "loss: 0.697706  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.725865, Avg loss: 0.937125 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 0.679977  [    0/ 8784]\n",
      "loss: 0.665758  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.728142, Avg loss: 0.934608 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 0.627701  [    0/ 8784]\n",
      "loss: 0.589413  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.727231, Avg loss: 0.931583 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 0.637601  [    0/ 8784]\n",
      "loss: 0.771396  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.734973, Avg loss: 0.928963 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 0.674564  [    0/ 8784]\n",
      "loss: 0.693866  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.734517, Avg loss: 0.939173 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 0.512305  [    0/ 8784]\n",
      "loss: 0.530148  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.737250, Avg loss: 0.919914 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 0.504866  [    0/ 8784]\n",
      "loss: 0.476840  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.736339, Avg loss: 0.915439 \n",
      "\n",
      "CPU times: user 8.76 s, sys: 1.12 s, total: 9.88 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=lstm_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.94      1.00      0.97       340\n",
      "     Chinese       0.66      0.76      0.71        38\n",
      "       Czech       0.62      0.16      0.25        96\n",
      "       Dutch       0.78      0.35      0.49        51\n",
      "     English       0.66      0.87      0.75       573\n",
      "      French       0.00      0.00      0.00        39\n",
      "      German       0.56      0.40      0.46       121\n",
      "       Greek       0.67      0.59      0.62        34\n",
      "       Irish       0.73      0.22      0.33        37\n",
      "     Italian       0.57      0.72      0.64       128\n",
      "    Japanese       0.81      0.84      0.83       156\n",
      "      Korean       0.20      0.10      0.13        10\n",
      "      Polish       0.48      0.46      0.47        26\n",
      "  Portuguese       0.00      0.00      0.00         9\n",
      "     Russian       0.85      0.85      0.85       458\n",
      "    Scottish       0.00      0.00      0.00        17\n",
      "     Spanish       0.52      0.32      0.40        50\n",
      "  Vietnamese       1.00      0.08      0.14        13\n",
      "\n",
      "    accuracy                           0.74      2196\n",
      "   macro avg       0.56      0.43      0.45      2196\n",
      "weighted avg       0.72      0.74      0.71      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(lstm_net, test_dataloader, \"mps\")\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "gru_net = SurnamesAutobotRNNClassifier(\n",
    "    rnn_cls=nn.GRU,\n",
    "    num_embeddings=len(surnames_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_size=64,\n",
    "    vector_size=surnames_dataset.vocab.max_len,\n",
    "    num_classes=len(surnames_dataset.labeler.classes_),\n",
    ").to(\"mps\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_net.parameters(), lr=0.0015)\n",
    "\n",
    "train_dataloader = DataLoader(train_surnames_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_surnames_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss:     inf  [    0/ 8784]\n",
      "loss: 2.811437  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.788394 \n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 2.771024  [    0/ 8784]\n",
      "loss: 2.729220  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.709271 \n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 2.710329  [    0/ 8784]\n",
      "loss: 2.656490  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.638715 \n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 2.631357  [    0/ 8784]\n",
      "loss: 2.600608  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.577541 \n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 2.534152  [    0/ 8784]\n",
      "loss: 2.549181  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.524147 \n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 2.520626  [    0/ 8784]\n",
      "loss: 2.505927  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.478398 \n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 2.481509  [    0/ 8784]\n",
      "loss: 2.435123  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.439397 \n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 2.395368  [    0/ 8784]\n",
      "loss: 2.308847  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.406757 \n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 2.361048  [    0/ 8784]\n",
      "loss: 2.345069  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.379277 \n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 2.321707  [    0/ 8784]\n",
      "loss: 2.267527  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.356812 \n",
      "\n",
      "Epoch 11\n",
      "--------------------------------\n",
      "loss: 2.399378  [    0/ 8784]\n",
      "loss: 2.308001  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.338106 \n",
      "\n",
      "Epoch 12\n",
      "--------------------------------\n",
      "loss: 2.324490  [    0/ 8784]\n",
      "loss: 2.348723  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.322937 \n",
      "\n",
      "Epoch 13\n",
      "--------------------------------\n",
      "loss: 2.230496  [    0/ 8784]\n",
      "loss: 2.222901  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.310603 \n",
      "\n",
      "Epoch 14\n",
      "--------------------------------\n",
      "loss: 2.309508  [    0/ 8784]\n",
      "loss: 2.229013  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.300486 \n",
      "\n",
      "Epoch 15\n",
      "--------------------------------\n",
      "loss: 2.220802  [    0/ 8784]\n",
      "loss: 2.280614  [ 6400/ 8784]\n",
      "Test Error: \n",
      " Accuracy: 0.260929, Avg loss: 2.292386 \n",
      "\n",
      "CPU times: user 10min 24s, sys: 3min 17s, total: 13min 41s\n",
      "Wall time: 10min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = common_train(\n",
    "    epochs=15,\n",
    "    model=gru_net,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    verbose=50,\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.00      0.00       340\n",
      "     Chinese       1.00      0.00      0.00        38\n",
      "       Czech       1.00      0.00      0.00        96\n",
      "       Dutch       1.00      0.00      0.00        51\n",
      "     English       0.26      1.00      0.41       573\n",
      "      French       1.00      0.00      0.00        39\n",
      "      German       1.00      0.00      0.00       121\n",
      "       Greek       1.00      0.00      0.00        34\n",
      "       Irish       1.00      0.00      0.00        37\n",
      "     Italian       1.00      0.00      0.00       128\n",
      "    Japanese       1.00      0.00      0.00       156\n",
      "      Korean       1.00      0.00      0.00        10\n",
      "      Polish       1.00      0.00      0.00        26\n",
      "  Portuguese       1.00      0.00      0.00         9\n",
      "     Russian       1.00      0.00      0.00       458\n",
      "    Scottish       1.00      0.00      0.00        17\n",
      "     Spanish       1.00      0.00      0.00        50\n",
      "  Vietnamese       1.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.26      2196\n",
      "   macro avg       0.96      0.06      0.02      2196\n",
      "weighted avg       0.81      0.26      0.11      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = get_y_test_y_pred(gru_net, test_dataloader, \"mps\")\n",
    "\n",
    "print(metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    target_names=surnames_dataset.labeler.classes_,\n",
    "    zero_division=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.LSTM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    accuracy                           0.74      2196\n",
    "   macro avg       0.56      0.43      0.45      2196\n",
    "weighted avg       0.72      0.74      0.71      2196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.GRU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    accuracy                           0.26      2196\n",
    "   macro avg       0.96      0.06      0.02      2196\n",
    "weighted avg       0.81      0.26      0.11      2196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.RNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    accuracy                           0.74      2196\n",
    "   macro avg       0.53      0.43      0.45      2196\n",
    "weighted avg       0.71      0.74      0.71      2196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: nn.LSTM предпочтительнее использовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPopRjm9A6la5QJRG/PWjfN",
   "collapsed_sections": [],
   "name": "blank__07_rnn_1_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
