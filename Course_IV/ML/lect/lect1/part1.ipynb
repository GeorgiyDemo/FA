{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 1 (часть 1): Введение в искусственные нейронные сети и глубокое обучение (deep learning)\n",
    "\n",
    "__Автор: Сергей Вячеславович Макрушин__ e-mail: SVMakrushin@fa.ru \n",
    "\n",
    "Финансовый универсиет, 2021 г. \n",
    "\n",
    "При подготовке лекции использованы материалы:\n",
    "* ...\n",
    "\n",
    "V 0.4 04.02.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделы: <a class=\"anchor\" id=\"разделы\"></a>\n",
    "* [Базовые понятия, персептрон](#персептрон)\n",
    "* [Современные методы обучения нейронной сети и обратное распространение ошибки](#современные-методы)\n",
    "* [Вторая весна ИИ и глубокое обучение](#вторая-весна)\n",
    " \n",
    "-\n",
    "\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовые понятия, персептрон <a class=\"anchor\" id=\"персептрон\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "﻿<style>\r\n",
       "\r\n",
       "\r\n",
       "b.n {\r\n",
       "    font-weight: normal;        \r\n",
       "}\r\n",
       "\r\n",
       "b.grbg {\r\n",
       "    background-color: #a0a0a0;      \r\n",
       "}\r\n",
       "\r\n",
       "b.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "b.b {    \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "b.g {\r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "// add your CSS styling here\r\n",
       "\r\n",
       "list-style: none;\r\n",
       "\r\n",
       "ul.s {\r\n",
       "//    list-style-type: none;\r\n",
       "    list-style: none;\r\n",
       "//    background-color: #ff0000;  \r\n",
       "//    color: #ffff00;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;\r\n",
       "}\r\n",
       "\r\n",
       "li.t {\r\n",
       "    list-style: none;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "*.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "li.t:before {\r\n",
       "    content: \"\\21D2\";    \r\n",
       "//    content: \"►\";\r\n",
       "//    padding-left: -1.2em;    \r\n",
       "    text-indent: -1.2em;    \r\n",
       "    display: block;\r\n",
       "    float: left;\r\n",
       "    \r\n",
       "    \r\n",
       "//    width: 1.2em;\r\n",
       "//    color: #ff0000;\r\n",
       "}\r\n",
       "\r\n",
       "i.m:before {\r\n",
       "    font-style: normal;    \r\n",
       "    content: \"\\21D2\";  \r\n",
       "}\r\n",
       "i.m {\r\n",
       "    font-style: normal; \r\n",
       "}    \r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "/* em {\r\n",
       "    font-style: normal; \r\n",
       "} */\r\n",
       "\r\n",
       "\r\n",
       "em.bl {\r\n",
       "    font-style: normal;     \r\n",
       "    font-weight: bold;        \r\n",
       "}\r\n",
       "\r\n",
       "/* em.grbg {\r\n",
       "    font-style: normal;         \r\n",
       "    background-color: #a0a0a0;      \r\n",
       "} */\r\n",
       "\r\n",
       "em.cr {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cb {    \r\n",
       "    font-style: normal;         \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cg {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "\r\n",
       "em.qs {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.qs::before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #ff0000;    \r\n",
       "    content: \"Q:\";  \r\n",
       "}\r\n",
       "\r\n",
       "em.an {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.an:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"A:\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.nt {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.nt:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Note:\";  \r\n",
       "}    \r\n",
       "    \r\n",
       "em.ex {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.ex:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #00ff00;    \r\n",
       "    content: \"Ex:\";  \r\n",
       "} \r\n",
       "    \r\n",
       "em.df {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.df:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Def:\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.pl {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.pl:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"+\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.mn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.mn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"-\";  \r\n",
       "}        \r\n",
       "\r\n",
       "em.plmn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.plmn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\00B1\";\\\\\"&plusmn;\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.hn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.hn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\21D2\";\\\\\"&rArr;\";  \r\n",
       "}     \r\n",
       "    \r\n",
       "\r\n",
       "#cssTableCenter td, th \r\n",
       "{\r\n",
       "    text-align: center; \r\n",
       "    vertical-align: middle;\r\n",
       "}\r\n",
       "\r\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стиль для оформления презентации\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"file:./lec_v2.css\")\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em class=\"df\"></em> __Искусственная нейронная сеть__ (ИНС, artificial neural network, ANN) - математическая модель, а также её программное или аппаратное воплощение, разработанная под влиянием изучения организации и функционирования биологических нейронных сетей - сетей нервных клеток живого организма.\n",
    "\n",
    "ИНС представляет собой систему (сеть) соединённых и взаимодействующих между собой простых вычислительных элементов (искусственных нейронов), которые, в общем случае, умеют:\n",
    "* реагировать на входной сигнал, возвращая реакцию на него\n",
    "* хранить некоторые параметры, обеспечивающие вариативность реакции на входной сигнал\n",
    "* обучаться - определенным образом менять свои параметры в ходе выполнения операции обучения\n",
    "\n",
    "Каждый искуственный нейрон обычно достаточно просто устроен и явлется узлом в искуственной нейронной сети. В сети он имеет дело только с сигналами, которые он периодически получает, и сигналами, которые он периодически посылает другим нейронам. Несмотря на простоту устройства искуственные нейроны будучи соединёнными в достаточно большую сеть с управляемым взаимодействием способны решать сложные задачи, многие из которых, до использования ИНС решать не удавалось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>     \n",
    "    <img src=\"./img/ann_1.png\" alt=\"Принципиальная схема устройства нейрона в живых организмах\" style=\"width: 500px;\"/>\n",
    "    <strong>Принципиальная схема устройства нейрона в живых организмах</strong>\n",
    "</center>\n",
    "\n",
    "* __Модели нейронов, используемые в современных ИНС, нацеленных на решение задач машинного обучения, в большинстве аспектов очень сильно отличаются от нейронов и их принципов работы в живых организмах!__ \n",
    "* Существет совершенно отдельное направление исследований, нацеленное на моделирование работы нейронов в живых организмах, далее мы об этом направлении говорить не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Очень краткая история области исследований__\n",
    "\n",
    "* 1943 - Уоренн Маккалок и Уолтер Питтс формализуют __понятие нейронной сети__.\n",
    "* 1949 - Дональд Хебб предлагает первый алгоритм, который может использоваться для __обучения искуственных нейронных сетей__.\n",
    "\n",
    "<center>     \n",
    "    <img src=\"./img/ml_1.png\" alt=\"Машинное обучение: альтернативная парадигма программирования\" style=\"width: 400px;\"/>\n",
    "    <strong>Машинное обучение: альтернативная парадигма программирования</strong>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1958 - Фрэнк Розенблатт изобретает __однослойный перцептрон__ и демонстрирует его способность __решать задачи классификации__. \n",
    "* 1960 год - Бернард Уидроу c Тедом Хоффом разработали __Адалин__ (ADALINE - Adaptive Linear Neuron, позже расшифровывался как Adaptive Linear Element), новый тип одноуровневой ИНС, отличавшийся от перспторна способом обучения. Адалин был построен на базе принципиально новых физических вычислительных элементов - мемисторах (разработан Уидроу и Хоффом). Сейчас Адалин (адаптивный сумматор) является стандартным элементом многих систем обработки сигналов.\n",
    "\n",
    "\n",
    "<center>     \n",
    "    <img src=\"./img/ann_2.png\" alt=\"Схема работы Перцептрона\" style=\"width: 500px;\"/>\n",
    "    <strong>Схема работы Перцептрона</strong>\n",
    "</center>\n",
    "\n",
    "Основной инновацией Розенблатта была разработка __алгоритма обучения перцептрона__:\n",
    "* изначально веса инициализируются случайным образом\n",
    "* поочередно берется один обучающий пример, включающий набор входов $x_i$ и верное значение $y$\n",
    "* для ошибочных предсказаний $\\hat{y}$:\n",
    "    * веса увеличиваюстя, если $\\hat{y}=0$, а $y=1$\n",
    "    * веса уменьшаются, если $\\hat{y}=1$, а $y=0$\n",
    "* процедура повторяется до исчезновения ошибок\n",
    "\n",
    "Для решения более сложных задач из перцептронов создается нейронная сеть:\n",
    "* для получения нескольких результатов __нейроны организуются в слой__ (layer) содержащий столькок перцептронов, сколько требуется выходов\n",
    "* выходы одного слоя могут использоваться в качестве входнов следующего слоя - многослойные нейронные сети\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/ann_4.png\" alt=\"Слой нейронной сети\" style=\"width: 300px;\"/>\n",
    "    <strong>Слой нейронной сети</strong>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Перцептрон и проблема линейно неразделимых множеств__\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/ann_5.png\" alt=\"Пример линейно разделимых множества объектов\" style=\"width: 300px;\"/>\n",
    "    <strong>Пример линейно разделимых множеств объектов</strong>        \n",
    "</center>\n",
    "\n",
    "Общий вид перцептрона:\n",
    "$$\\hat{y} = f(w_0 + w_1 x_1 + \\ldots + w_n x_n) = f (\\pmb{w}^T \\pmb{x})$$, \n",
    "где $\\pmb{x}=(1, x_1, \\ldots, x_n)$\n",
    "\n",
    "В качестве фунции активации $f$ могут использоваться функции:\n",
    "* Сигмоид (логистическая функция): $\\sigma(z)=\\frac{e^z}{1+e^z}$\n",
    "* Гиперболический тангенс: $tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$\n",
    "* Единичная ступенчатая функция (функция Хевисайда): $step(z) = \\begin{cases} \\ \\ 1 \\text{ , if } x > 0 \\\\ \\ \\ 0 \\text{ , if } x \\le 0 \\end{cases}$\n",
    "* Rectified linear unit (вентиль): $ReLU(z)=\\begin{cases} z \\text{ , if } z > 0 \\\\ 0 \\text{ , if } otherwise \\\\ \\end{cases}$\n",
    "* $sign(z) = \\begin{cases} \\ \\ 1 \\text{ , if } x > 0 \\\\ \\ \\ 0 \\text{ , if } x = 0 \\\\ -1 \\text{ , if }  x < 0 \\end{cases}$\n",
    "\n",
    "Большая коллекция функций активации: https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "Требования к функциям активации:\n",
    "* функция должна быть монотонной (обычно монотонно не убывающая)\n",
    "* иметь первую производную почти всюду (необходимо для обратного распространения ошибки при обучении нейронной сети)\n",
    "\n",
    "<center>\n",
    "    <img src=\"./img/ann_6.png\" alt=\"Графиики функций активации\" style=\"width: 600px;\"/>\n",
    "    <strong>Графиики функций активации</strong>            \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <img src=\"./img/ann_7.png\" alt=\"Пример линейно разделимых множества объектов\" style=\"width: 600px;\"/>\n",
    "    <strong>Пример поиска границы разделяющей два класса с помощью перцептрона с $f=\\sigma$</strong>                \n",
    "</center>\n",
    "\n",
    "Один перцептрон (с любой функцией активации) может научиться классифицировать только линейно раздлемые множества объектов.\n",
    "\n",
    "Предположим мы имеем два признака (features) на входе перцептрона: $𝑥_1 \\text{, } 𝑥_2 \\in \\{0, 1\\}$, т.е. имеем четрые точки:\n",
    "<table>\n",
    "<colgroup>\n",
    "<col width=\"33%\">\n",
    "<col width=\"33%\">\n",
    "<col width=\"34%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">$x_1$</th>\n",
    "<th class=\"head\">$x_2$</th>\n",
    "<th class=\"head\">$y$</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td>0</td>\n",
    "<td>0</td>\n",
    "<td>...</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>1</td>\n",
    "<td>1</td>\n",
    "<td>...</td>\n",
    "</tr>    \n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td>0</td>\n",
    "<td>1</td>\n",
    "<td>...</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>1</td>\n",
    "<td>0</td>\n",
    "<td>...</td>\n",
    "</tr>     \n",
    "</tbody>    \n",
    "</table>    \n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/ann_8.png\" alt=\"Пример классификации перцептроном логических функций\" style=\"width: 600px;\"/>\n",
    "    <strong>Пример классификации перцептроном логических функций</strong>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 1969 год - Марвин Мински публикует формальное __доказательство ограниченности перцептрона__ и показывает, что он неспособен решать некоторые задачи (проблема «чётности» и «один в блоке»), связанные с инвариантностью представлений. В частности, __один перцептрон не может реализовать функцию XOR__ (исключающее или).\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/ann_9.png\" alt=\"Проблема классификации перцептроном логическоц функции XOR\" style=\"width: 250px;\"/>\n",
    "    <strong>Проблема классификации перцептроном логической функции XOR</strong>    \n",
    "</center>\n",
    "\n",
    "Сам М. Мински показал, что __XOR может быть реализован многослойной нейронной сетью из перцептронов__. Нелинейная функция активации является кртичиески важным элментом перцептрона, без нее линейная комбинация перцептронов позволяла бы строить только линейные разделяющие поверхности. \n",
    " \n",
    "<center> \n",
    "    <img src=\"./img/ann_10.png\" alt=\"Классификации логическоц функции XOR многослойным перцептроном\" style=\"width: 600px;\"/>\n",
    "    <strong>Классификации логическоц функции XOR многослойным перцептроном</strong>        \n",
    "</center>\n",
    "\n",
    "\n",
    "Несмотря на возможность работы многослойных перцптронов о __механизм обучения, предложенный Розенблаттом, не позволяет обучить необходимую многослойную сеть__. После данных результатов интерес к нейронным сетям резко спадает, наступает период, позже называнный __\"Первой зимой искуственного интеллекта\"__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Современные методы обучения нейронной сети и обратное распространение ошибки <a class=\"anchor\" id=\"современные-методы\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Обратное распространение ошибки (backpropagation)__ - алгоритм, позволяющий производить обучение многослойных нейронных сетей. \n",
    "\n",
    "* 1974 - Пол Дж. Вербос __изобретает алгоритм обратного распространения ошибки__ для обучения многослойных перцептронов. Изобретение не привлекло особого внимания.\n",
    "* 1982 - после периода забвения, интерес к нейросетям вновь возрастает (__\"весна искуственного интеллекта\"__). Дж. Хопфилд показал, что нейронная сеть с обратными связями может представлять собой систему, минимизирующую энергию (так называемая сеть Хопфилда). Кохоненом представлены модели сети, обучающейся без учителя (нейронная сеть Кохонена), решающей задачи кластеризации, визуализации данных (самоорганизующаяся карта Кохонена) и другие задачи предварительного анализа данных.\n",
    "* 1986 - Дэвидом И. Румельхартом, Дж. Е. Хинтоном и Рональдом Дж. Вильямсом переоткрыт и существенно развит __метод обратного распространения ошибки__. Начался взрыв интереса к обучаемым нейронным сетям.\n",
    "\n",
    "Использованием алгоритма обратного распространение ошибки позволяет:\n",
    "* __реализовывать__ функцию XOR и другие более __сложные функции__ с помощью многослойных нейронных сетей\n",
    "* эффективно обрабатывать __большое количество обучающих примеров__\n",
    "* обучать __сложные архитектуры нейронных сетей__\n",
    "Данный алгоритм до сих пор является одним из ключевых решений в технологиях искуственный нейронных сетей./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Приниципиальная логика обучения нейронной сети__\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/ann_11.png\" alt=\"Пример многослойного перцептрона \" style=\"width: 500px;\"/>\n",
    "    <strong>Пример многослойного перцептрона (с двумя скрытыми слоями)</strong>\n",
    "</center>\n",
    "\n",
    "* У нас есть набор данных $D$, состоящий из пар $(\\pmb{x}, \\pmb{y})$, где $\\pmb{x}$ - признаки, а $\\pmb{y}$ - правильный ответ. \n",
    "* Модель сети $f_L$, имеющей $L$ слоев с весами $\\pmb{\\theta}$ (совокупность весов нейронов из всех слоев) на этих данных делает некоторые предсказания $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})$\n",
    "* Задана функция ошибки $E$, которую можно подсчитать на каждом примере: $E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$ (например, это может быть квадрат или модуль отклонения $\\hat{\\pmb{y}}$ от $\\pmb{y}$ в случае регрессии или перекрестная энтропия в случае классификации)\n",
    "* Тогда суммарная ошибка на наборе данных $D$ будет функцией от параметров модели: $E(\\pmb{\\theta})$ и определяется как $E(\\pmb{\\theta})=\\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/main_cycle_p1_v1.png\" alt=\"Приниципиальная логика обучения нейронной сети\" style=\"width: 600px;\"/>\n",
    "    <strong>Приниципиальная логика обучения нейронной сети</strong>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проблема обучения модели нейронной сети__\n",
    "\n",
    "* <em class=\"nt\"></em> __основная проблема__ это не применение модели к входным данным $\\pmb{x}$ и оцнка ошибки на правильных ответах $\\pmb{y}$, а __обучение модели__ (опредление наилучших параметров модели $\\pmb{\\theta}$). \n",
    "     * В случае нейронной сети обучение сводится к поиску весов слоев сети $\\pmb{\\theta}=(\\pmb{w}_1, \\ldots, \\pmb{w}_L)$, которые в совокупности являются параметрами модели $\\pmb{\\theta}$.\n",
    "\n",
    "* Формально: цель обучения - найти оптимальное значение параметров $\\theta^{*}$, минимизирующих ошибку на обучающией выборке $D$: \n",
    "$$\\theta^{*} = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ E(\\pmb{\\theta}) = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "* Т.е. задача обучения сводится к задаче оптимизации.\n",
    "    * <em class=\"nt\"></em> На самом деле __все сложнее__: хороший результат на $D$ может плохо обобщаться (модель может давать низкое качество на другой выборке из той же генеральной совокупности) - __проблема переобучения__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача оптимизации__\n",
    "\n",
    "* Задача: корректировка весов сети (параметров модели $\\pmb{\\theta}$) на основе информации об ошибке на обучающих примерах $E(\\hat{\\pmb{y}}, \\pmb{y})$.\n",
    "    * Решение: использовать методы оптимизации, основанные на __методе градиентного спуска__.\n",
    "    \n",
    "\n",
    "* __Метод градиентныого спуска__ - метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль градиента. В нашем случае шаг метода градиентного спуска выглядит следующим образом:\n",
    "$$\\pmb{\\theta}_t = \\pmb{\\theta}_{t-1}-\\gamma\\nabla_\\theta E(\\pmb{\\theta}_{t-1}) = \\pmb{\\theta}_{t-1}-\\gamma \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} \\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "\n",
    "* <em class=\"nt\"></em> Выполнение на каждом шаге градиентого спуска суммирование по всем $(\\pmb{x}, \\pmb{y}) \\in D$ __обычно слшиком неэффективно__\n",
    "\n",
    "\n",
    "* Для выпуклых функций __задача локальной оптимизации__ - найти локальный минимум (максимум) автоматически превращается в __задачу глобальной оптимизации__ - найти точку, в которой достигается наименьшее (наибольшее) значение функции, то есть самую низкую (высокую) точку среди всех.\n",
    "* Оптимизировать веса одного перцептрона - выпуклая задача, но __для большой нейронной сети  целевая функция не является выпуклой__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_15.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 500px;\"/><br/>\n",
    "    <b>Пример работы градиентного спуска для функции двух переменных</b>    \n",
    "</center>\n",
    "\n",
    "* У нейронных сетей функция ошибки может задавать __очень сложный ландшафт__ с огромным числом локальных максимумов и минимумов. Это свойство необходимо для обеспечения выразительности нейронных сетей, позволяющей им решать так много разных задач.\n",
    "\n",
    "\n",
    "* <em class=\"nt\"></em> для использования методов, основанных на методе градиентного спуска __необходимо знать градиент функции потерь по параметрам модели__: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$. Этот градиент определяет вектор (\"направление\") изменения параметров.\n",
    "\n",
    "\n",
    "Прямой проход (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}}$\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/ann_12.png\" alt=\"Пример прямого прохода\" style=\"width: 500px;\"/>\n",
    "    <strong>Пример прямого прохода</strong>    \n",
    "</center>\n",
    "\n",
    "\n",
    "Обратное распространение ошибки (backpropagation):\n",
    "* расчитывается ошибка между выходным вектором сети $\\hat{\\pmb{y}}$ и правильным ответом обучающего примера $\\pmb{y}$\n",
    "* ошибка распростаняется от результата к источнику (в обратную сторону) для корректировки весов\n",
    "\n",
    "<center>\n",
    "    <img src=\"./img/ann_13.png\" alt=\"Пример прямого прохода\" style=\"width: 500px;\"/>\n",
    "    <strong>Пример обратного распространения ошибки</strong>    \n",
    "</center>\n",
    "\n",
    "В результате мы рассчитываем градиент функции потерь по параметрам модели: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$ и с его помощью проводим обучаение (оптимизацию) весов сети $\\pmb{\\theta}$ на основе полученных ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Метод обратного распространения ошибки__ позволяет обучать многослойные сети, что позволяет решать сложные задачи.\n",
    "<center>   \n",
    "    <img src=\"./img/db_1.png\" alt=\"Пример\" style=\"width: 750px;\"/>\n",
    "    <img src=\"./img/db_2.png\" alt=\"Пример\" style=\"width: 550px;\"/>\n",
    "    <img src=\"./img/db_3.png\" alt=\"Пример\" style=\"width: 550px;\"/>\n",
    "    <img src=\"./img/db_4.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "    <strong>Пример: решение задачи двухклассовой классификации сетями с разными праметрами слоев</strong>        \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вторая весна ИИ и глубокое обучение <a class=\"anchor\" id=\"вторая-весна\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вторая зима ИИ__\n",
    "\n",
    "* До 1998 были предложены некоторые  очень эффективные методы в облести ИНС:\n",
    "    * Обратное распространение ошибки (backpropagation)\n",
    "    * Recurrent Long-Short Term Memory Networks (LSTM)\n",
    "    * Распрознавание символов с помощью Convolutional Neural Networks (CNN)\n",
    "\n",
    "* Однако, в это время стали очень популярны некоторые альтернативные мотоды машинного обучения (в частности, SVM и т.д.) \n",
    "    * они обеспечивали аналогичное качество на тех же задачха\n",
    "    * не удавалось строить ИНС глубже нескольких слоев\n",
    "    * Kernel Machines использовали намного меньше эвристики и имели отличные математические доказательства обобщающией способности \n",
    "* В результате сообщество специалистов из области AI снова отвернулось от ИНС.\n",
    "\n",
    "Neural Network and Deep Learning problems:\n",
    "* Lack of processing power\n",
    "    * No GPUs at the time\n",
    "* Lack of data\n",
    "    * No big, annotated datasets at the time\n",
    "* Overfitting\n",
    "    * Because of the above, models could not generalize all that well\n",
    "* Vanishing gradient\n",
    "    * While learning with NN, you need to multiply several numbers 𝑎1 ∙ 𝑎2 ∙ ⋯ ∙ 𝑎𝑛.\n",
    "    * If all are equal to 0.1, for 𝑛 = 10 the result is 0.0000000001, too small for any learning\n",
    "* Experimentally, training multi-layer perceptrons was not that useful\n",
    "    * Accuracy didn’t improve with more layers\n",
    "* The inevitable question\n",
    "    * Are 1-2 hidden layers the best neural networks can do?\n",
    "    * Or is it that the learning algorithm is not really mature yet\n",
    "    \n",
    "... Deep Learning arrives    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 1990е и 2000е годы альтернативные методы поверхностного обучения в большинстве задач превосходили методы, основанные на нейронных сетях.\n",
    "\n",
    "Альтернативы нейронным сетям:\n",
    "* __Ядерные методы__ — это группа алгоритмов классификации, из которых наибольшую известность получил __метод опорных векторов__ (Support Vector Machine, SVM).\n",
    "* __Деревья решений__ и __случайные леса__ \n",
    "* __Градиентный бустинг__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Метод опорных векторов__\n",
    "\n",
    "Метод опорных векторов предназначен для решения задач классификации путем поиска хороших решающих границ, разделяющих два набора точек, принадлежащих разным категориям. Поиск таких границ в методе осуществляется в два этапа:\n",
    "\n",
    "1. Данные отображаются в новое пространство более высокой размерности, где граница может быть представлена как гиперплоскость.\n",
    "2. Хорошая решающая граница (разделяющая гиперплоскость) вычисляется путем максимизации расстояния от гиперплоскости до ближайших точек каждого класса, этот этап называют максимизацией зазора. Это позволяет обобщить классификацию новых образцов, не принадлежащих обучающему набору данных.\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/alt_met1.png\" alt=\"Принципиальная схема работы SVM\" style=\"width: 500px;\"/>\n",
    "    <strong>Принципиальная схема работы SVM</strong>     \n",
    "</center>\n",
    "\n",
    "На момент разработки метод опорных векторов:\n",
    "* демонстрировал лучшую производительность на простых задачах классификации\n",
    "* был одним из немногих методов машинного обучения, обладающих обширной теоретической базой и поддающихся серьезному математическому анализу\n",
    "* был хорошо понятным и легко интерпретируемым\n",
    "\n",
    "Но:\n",
    "* метод оказался трудно применимым к большим наборам данных\n",
    "* не дал хороших результатов для таких задач, как классификация изображений\n",
    "\n",
    "Так как SVM является поверхностным методом, для его применения к задачам распознавания требуется сначала вручную выделить представительную выборку (этот шаг называется конструированием признаков), что сопряжено со сложностями и чревато ошибками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Деревья решений__\n",
    "\n",
    "Деревья решений — это иерархические структуры, которые позволяют классифицировать входные данные или предсказывать выходные значения по заданным исходным значениям. Они легко визуализируются и интерпретируются. \n",
    "\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/alt_met2_.png\" alt=\"Принципиальная схема работы дерева решений\" style=\"width: 500px;\"/>\n",
    "    <strong>Принципиальная схема работы дерева решений</strong>     \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм \"случайный лес\" (Random Forest) предложил надежный и практичный подход к обучению на основе деревьев решений, включающий в себя создание большого количества специализированных деревьев решений с последующим объединение выдаваемых ими результатов.\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/alt_met3.png\" alt=\"Принципиальная схема работы алгоритма &quot;случайного леса&quot;\" style=\"width: 500px;\"/>\n",
    "    <strong>Принципиальная схема работы алгоритма \"случайного леса\"</strong>     \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод __градиентного бустинга__, во многом напоминает случайный лес, он основан на объединении слабых моделей прогнозирования, обычно — деревьев решений. Он использует градиентный бустинг, способ улучшения любой модели машинного обучения путем итеративного обучения новых моделей, специализированных для устранения слабых мест в предыдущих моделях. \n",
    "\n",
    "Применительно к деревьям решений градиентный бустинг позволяет получить модели, которые в большинстве случаев \n",
    "превосходят случайные леса при сохранении аналогичных свойств.\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/alt_met4.png\" alt=\"Принципиальная схема работы алгоритма градиентного бустинга\" style=\"width: 500px;\"/>\n",
    "    <strong>Принципиальная схема работы алгоритма градиентного бустинга</strong>     \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* К 2010 году __практически потерян интерес к искуственным нейронным сетям__ со стороны научного сообщества, работают небольшие группы энтузиастов (__2я зима искусственного интеллекта__)\n",
    "* Начиная с 2010 года появляются важные успехи в области применения искуственных нейронных стеей, связанные со следующими научными группами:\n",
    "    * Джеффри Хинтон (Geoffrey Hinton) из университета в Торонто\n",
    "    * Йошуа Бенгио (Yoshua Bengio) из университета в Монреале\n",
    "    * Ян Лекун (Yann LeCun) из университета в Нью-Йорке\n",
    "    * исследователи из научно-исследовательском институте искусственного интеллекта IDSIA в Швейцарии\n",
    "* В 2011 году Ден Киресан (Dan Ciresan) из IDSIA выиграл конкурс по классификации изображений с применением глубоких нейронных сетей, обучаемых на GPU: первый практический успех современного глубокого обучения\n",
    "* __Соревнование ImageNet__ - крупномасштабное распознавание образов. Классификации цветных изображений с высоким разрешением на 1000 разных категорий после обучения по выборке, включающей в себя 1,4 миллиона изображений. Для начала 2010х годов - очень сложная задача машинного обучения. В 2011 году модель-победитель, основанная на классических подходах к распознаванию образов, показала __точность лишь 74,3%__ .\n",
    "* В 2012 году команда Алекса Крижевски (Alex Krizhevsky), советником в которой был Джеффри Хинтон (Geoffrey Hinton), достигла __точности в 83,6%__ — значительный прорыв методов глубокого обучения.\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/deepnet_1_.png\" alt=\"Уровень ошибки в соревновании ImageNet по годам\" style=\"width: 900px;\"/>\n",
    "    <strong>Уровень ошибки в соревновании ImageNet по годам</strong>     \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <img src=\"./img/ann_21.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "    <strong>Вторая весна ИИ</strong>        \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой ступени развития достигло глубокое обучение:\n",
    "\n",
    "* классификация изображений на уровне человека;\n",
    "* распознавание речи на уровне человека;\n",
    "* распознавание рукописного текста на уровне человека;\n",
    "* улучшение качества машинного перевода с одного языка на другой;\n",
    "* улучшение качества машинного чтения текста вслух;\n",
    "* появление цифровых помощников, таких как Google Now и Amazon Alexa;\n",
    "* управление автомобилем на уровне человека;\n",
    "* повышение точности целевой рекламы, используемой компаниями Google, Baidu и Bing;\n",
    "* повышение релевантности поиска в интернете;\n",
    "* появление возможности отвечать на вопросы, заданные вслух;\n",
    "* игра в Го сильнее человека.\n",
    "\n",
    "Но, скорее всего, еще долгое время будут оставаться недостижимым решение таких задач, как:\n",
    "* перевод между произвольными языками на уровне человека;\n",
    "* понимание естественного языка на уровне человека\n",
    "\n",
    "Не стоит всерьез воспринимать разговоры об интеллекте на уровне человека"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Глубокое обучение__ — это особый раздел машинного обучения: новый подход к поиску представления данных, делающий упор на изучение последовательных слоев (или уровней) все более значимых представлений. \n",
    "\n",
    "* Под __глубиной__ в глубоком обучении не подразумевается __многослойное представление данных__. \n",
    "    * __Количество слоев__, на которые делится модель данных, называют __глубиной модели__.\n",
    "    * Другими подходящими названиями для этой области машинного обучения могли бы служить: многослойное обучение и иерархическое обучение.\n",
    "    * Под глубиной в глубоком обучении не подразумевается более глубокое понимание, достигаемое этим подходом.\n",
    "\n",
    "* Современное глубокое обучение часто вовлекает в процесс __десятки и даже сотни последовательных слоев__ представления.\n",
    "    * Все они __автоматически определяются под воздействием обучающих данных__. \n",
    "    * Другие подходы к машинному обучению __ориентированы на изучении одного-двух слоев представления данных__, по этой причине их иногда называют __поверхностным обучением__.\n",
    "    \n",
    "<center> \n",
    "    <img src=\"./img/deepnet_2.png\" alt=\"Увеличение глубины нейронных сетей\" style=\"width: 500px;\"/>\n",
    "    <strong>Увеличение глубины нейронных сетей</strong>     \n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличительные черты глубокого обучения\n",
    "\n",
    "Основные причины быстрого взлета глубокого обучения:\n",
    "* лучшая производительность во многих задачах\n",
    "* упрощение решения проблем, за счет полной автоматизации важнейшего шаг в машинном обучении: конструирования признаков (раньше он выполнялся вручную)\n",
    "    * Методы поверхностного обучения — включали в себя преобразование входных данных только в одно или два последовательных пространства, обычно посредством простых преобразований.\n",
    "    * Однако точные представления, необходимые для решения сложных задач, обычно нельзя получить такими способами. Приходилось прилагать большие усилия, чтобы привести исходные данные к виду, более пригодному для обработки этими методами: приходилось __вручную улучшать слой представления своих данных__. Это называется __конструированием признаков__.\n",
    "    * Глубокое обучение, напротив, полностью автоматизирует этот шаг: с применением методов глубокого обучения все признаки извлекаются за один проход, без необходимости конструировать их вручную.\n",
    "* возможность эффективно использовать специализированное высокопроизводительное оборудование (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение задачи конструирования признаков\n",
    "\n",
    "* Методы поверхностного обучения используют преобразование входных данных только в одно или два последовательных пространства, обычно посредством простых преобразований.\n",
    "* Однако точные представления, необходимые для решения сложных задач, обычно нельзя получить такими способами. Приходилось __вручную улучшать слой представления своих данных__ - прилагать большие усилия, чтобы привести исходные данные к виду, более пригодному для обработки этими методами. Это называется __конструированием признаков__ (feature engineering).\n",
    "----\n",
    "* Глубокое обучение полностью автоматизирует процесс конструированием признаков: все признаки извлекаются за один проход, без необходимости конструировать их вручную.\n",
    "* Можно ли многократно применить методы поверхностного обучения для имитации эффекта глубокого обучения? \n",
    "    * Проблема: оптимальный слой первого представления в трехслойной модели не является оптимальным первым слоем в однослойной или двухслойной модели\n",
    "    * В глубоком обучении модель может __исследовать все слои представления вместе и одновременно__, а не последовательно (последовательное исследование также называют «жадным»). Когда модель корректирует один из своих внутренних признаков, все прочие признаки, зависящие от него, автоматически корректируются в соответствии с изменениями, без вмешательства человека. Все контролируется единственным сигналом обратной связи.\n",
    "* Методика глубокого обучения обладает двумя важными характеристиками: \n",
    "    * она поэтапно, послойно конструирует все более сложные представления \n",
    "    * совместно исследует промежуточные представления, благодаря чему каждый слой обновляется в соответствии с потребностями представления слоя выше и потребностями слоя ниже. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Почему глубокое обучение начало приносить плоды и активно использоваться только после 2010 г?__\n",
    "\n",
    "* Еще в 1989 году были известны две ключевые идеи глубокого обучения:\n",
    "    * алгоритм обратного распространения ошибки\n",
    "    * сверточные нейронные сети\n",
    "    * в 1997 г. был предложен алгоритм долгой краткосрочной памяти (Long Short-Term Memory, LSTM)\n",
    "    \n",
    "В целом, прогресс глубокого обучения объясняется тремя основными факторами:\n",
    "* __производительность оборудования__\n",
    "* __доступность наборов данных и тестов__\n",
    "* __алгоритмические достижения__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Производительность оборудования__\n",
    "\n",
    "* Между 1990 и 2010 годами быстродействие стандартных процессоров выросло примерно в 5000 раз (закон Мура).\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/deepnet_3_2.png\" alt=\"Закон Мура\" style=\"width: 500px;\"/>\n",
    "    <strong>Закон Мура</strong>     \n",
    "</center> \n",
    "\n",
    "* Но: мощности соврменного ноутбука недостаточно, чтобы обучить типичные модели глубокого обучения, используемые для распознавания образов или речи, они требуют на порядки больше вычислительной мощности.\n",
    "* В течение 2000х такие компании, как NVIDIA и AMD, вложили миллионы долларов в разработку быстрых процессоров с массовым параллелизмом: __графических процессоров - Graphical Processing Unit (GPU)__ для поддержки графики все более реалистичных видеоигр\n",
    "* В 2007 году компания NVIDIA выпустила __CUDA__ (Compute Unified Device Architecture) - программный интерфейс для линейки своих GPU, позволяющий использовать их для вычислений общего назначения, а не только для специализированных задач компьютерной графики GPGPU (General-Purpose computing on Graphics Processing Units).\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/deepnet_3.png\" alt=\"Различия в архитектурах CPU и GPU\" style=\"width: 500px;\"/>\n",
    "    <strong>Различия в архитектурах CPU и GPU</strong>     \n",
    "</center> \n",
    "\n",
    "* Теперь в различных задачах, допускающих возможность массового распараллеливания вычислений несколько GPU могут заменить мощные кластеры на обычных процессорах. Современный графический процессор NVIDIA TITAN X, (стоимостью ~ 1000 USD) имеет пиковую   роизводительность по выполнению операций с числами типа float32 __почти в 350 раз больше__ производительности современного ноутбука.\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/deepnet_3_3.png\" alt=\"Различия в производительности CPU и GPU\" style=\"width: 500px;\"/>\n",
    "    <strong>Различия в производительности CPU и GPU</strong>     \n",
    "</center> \n",
    "\n",
    "\n",
    "* __Глубокие нейронные сети допускают высокую степень распараллеливания__ т.к. выполняют в основном умножение множества маленьких матриц. \n",
    "* Ведется разработка __специализированных процессоров для решения задач глубокого обучения__, частного случая ASIC ( application-specific integrated circuit - «интегральная схема специального назначения»). Компания Google разработала и использует несколько поколений специализированных тензорных процессоров (Tensor Processing Unit, TPU), ориентированных на решение задач глубокого обучения: процессор с новой архитектурой, предназначенный для использования в глубоких нейронных сетях, который примерно в 10 раз  производительнее и энергоэффективнее чем топовые модели GPU.\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/deepnet_3_4.png\" alt=\"Google tensor processing unit (TPU)\" style=\"width: 500px;\"/>\n",
    "    <strong>Google tensor processing unit (TPU)</strong>     \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Доступность наборов данных и тестов__\n",
    "\n",
    "* Алгоритмы машинного обучения используют данные (в частности: обучающие наборы данных) для собственной настройки (обучения) и повышения качества работы. \n",
    "* Обучение решнию сложных задач требует очень больших объемов обучающих наборов данных.\n",
    "\n",
    "---\n",
    "Накоплению больших объемов обучающих выборок в последние десятилетия способствовали следующие факторы:\n",
    "* Экспоненциальный рост емкости устройств хранения информации, наблюдавшемуся в последние 20 лет (согласно закону Мура)\n",
    "* Бурный рост интернета, благодаря которому появилась возможность работать с очень большими объамами данных, в частности:\n",
    "    * собирать и накапливать данные\n",
    "    * распространять данные \n",
    "    * совместно обрабатывать (вручную и автоматически) данные \n",
    "\n",
    "Примеры публично доступных массивов данных:\n",
    "* База данных ImageNet — проект по созданию и сопровождению массивной базы данных аннотированных изображений, предназначенная для отработки и тестирования методов распознавания образов и машинного зрения. По состоянию на 2016 год в базу данных было записано около десяти миллионов URL с изображениями, которые прошли ручную аннотацию для ImageNet, в аннотациях перечислялись объекты, попавшие на изображение, и прямоугольники с их координатами.\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/deepnet_4.png\" alt=\"ImageNet\" style=\"width: 500px;\"/>\n",
    "    <strong>ImageNet</strong>     \n",
    "</center> \n",
    "\n",
    "* Википедия\n",
    "\n",
    "<center> \n",
    "<table>\n",
    "<tr>\n",
    "    <td><img src=\"./img/deepnet_5.png\" alt=\"Динамика количества статей в Википедии\" style=\"width: 500px;\"/></td>\n",
    "    <td><img src=\"./img/deepnet_6.png\" alt=\"Динамика количества статей в Википедии\" style=\"width: 200px;\"/></td>\n",
    "</tr>\n",
    "</table>\n",
    "    <strong>Динамика количества статей в Википедии (в разрезе языковых разделов)</strong>         \n",
    "</center>     \n",
    "    \n",
    "* Социальные сети    \n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/deepnet_7.png\" alt=\"Динамика количества пользователей Facebook\" style=\"width: 500px;\"/>\n",
    "    <strong>Динамика количества пользователей Facebook</strong>     \n",
    "</center> \n",
    "\n",
    "* И много-много всего...\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/deepnet_9.png\" alt=\"Каждую минту\" style=\"width: 500px;\"/>\n",
    "    <strong>Каждую минту...</strong>     \n",
    "</center> \n",
    "\n",
    "* Далее: интернет вещей (internet of things, IoT) — концепция вычислительной сети физических предметов (\"вещей\"), оснащённых встроенными технологиями для взаимодействия друг с другом или с внешней средой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вторая весна ИИ и глубокое обучение <a class=\"anchor\" id=\"вторая-весна\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Алгоритмические достижения в области глубокого обучения__\n",
    "\n",
    "Кроме оборудования и данных, до конца 2000-х нам не хватало надежного способа обучения очень глубоких нейронных сетей, как результат:\n",
    "* нейронные сети оставались очень неглубокими, имеющими один или два слоя представления; \n",
    "* <em class=\"hn\"></em> они не могли противостоять более совершенным поверхностным методам (методу опорных векторов и случайные леса). Основная __проблема__ заключалась в __распространении градиента через глубокие пакеты слоев__. Сигнал обратной связи, используемый для обучения нейронных сетей, __затухает по мере увеличения количества слоев__.\n",
    "\n",
    "<center> \n",
    "    <img src=\"./img/ann_14.png\" alt=\"Приниципиальная логика обучения нейронной сети\" style=\"width: 370px;\"/>\n",
    "    <strong>Приниципиальная логика обучения нейронной сети</strong>     \n",
    "</center>\n",
    "\n",
    "В районе 2010 г. появились некоторые простые, но важных алгоритмические усовершенствования, позволившие улучшить распространение градиента:\n",
    "* __улчшенные подходы к регуляризации__\n",
    "* __улучшенные схемы инициализации весов__\n",
    "* __улучшенные функции активации__\n",
    "* __улучшенные схемы оптимизации__ (такие как RMSProp и Adam)\n",
    "\n",
    "В последнее время были открыты еще более совершенные способы распространения градиента, с применением которых появилась возможность обучать с нуля модели с тысячами слоев в глубину. В частонсти:\n",
    "* пакетная нормализация\n",
    "* обходные связи \n",
    "* отделимые свертки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Спасибо за внимание!\n",
    "\n",
    "---\n",
    "### Технический раздел:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> next <em class=\"qs\"></em> qs line \n",
    "<br/> next <em class=\"an\"></em> an line \n",
    "<br/> next <em class=\"nt\"></em> an line \n",
    "<br/> next <em class=\"df\"></em> df line \n",
    "<br/> next <em class=\"ex\"></em> ex line \n",
    "<br/> next <em class=\"pl\"></em> pl line \n",
    "<br/> next <em class=\"mn\"></em> mn line \n",
    "<br/> next <em class=\"plmn\"></em> plmn line \n",
    "<br/> next <em class=\"hn\"></em> hn line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
